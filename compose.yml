# services:
#   article-grabber:
#     build: article-grabber

#   llm-model:
#     build: llm-model

services:
  # ollama:
  #   image: ollama/ollama:latest
  #   ports:
  #     - 11434:11434
  #   volumes:
  #     - .:/code
  #     - ./ollama/ollama:/root/.ollama
  #   container_name: ollama
  #   pull_policy: always
  #   tty: true
  #   restart: always
  #   networks:
  #     - ollama-docker
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  model:
    build: llm-model/
    ports:
      - 11434:11434
    volumes:
      - ./llm-model:/code
      - ./llm-model/ollama/ollama:/root/.ollama
    container_name: ollama
    pull_policy: always
    tty: true
    restart: always
    networks:
      - ollama-docker
    healthcheck:
      test: ["CMD-SHELL","ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  # ollama-setup:
  #     build: llm-model/
  #     depends_on:
  #       model:
  #         condition: service_healthy
  #     restart: "no"
  #     entrypoint: [ "bash", "-c", "/root/create_model.sh"]

networks:
  ollama-docker:
    external: false